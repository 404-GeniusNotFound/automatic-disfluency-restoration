{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":120972,"databundleVersionId":14460268}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":17.684008,"end_time":"2025-11-15T18:02:29.319511","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-15T18:02:11.635503","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Data Prep\nimport pandas as pd\nimport os\nimport re\n\nBASE_PATH = \"/kaggle/input/nppe-2-automatic-disfluency-restoration\"\nTRAIN_CSV_PATH = os.path.join(BASE_PATH, \"train.csv\")\nDISFLUENCY_CSV_PATH = os.path.join(BASE_PATH, \"unique_disfluencies.csv\")\nAUDIO_PATH = os.path.join(BASE_PATH, \"downloaded_audios\")\n\nprint(\"Loading data...\")\ntrain_df = pd.read_csv(TRAIN_CSV_PATH)\ndisfluency_df = pd.read_csv(DISFLUENCY_CSV_PATH)\ndisfluency_set = set(disfluency_df['disfluency'])\n\ndef create_clean_transcript(disfluent_text):\n    if not isinstance(disfluent_text, str): return \"\"\n    words = disfluent_text.split()\n    clean_words = [word for word in words if word not in disfluency_set]\n    return \" \".join(clean_words)\n\nprint(\"Creating clean transcripts...\")\ntrain_df['clean_transcript'] = train_df['transcript'].apply(create_clean_transcript)\ntrain_df['audio_file'] = train_df['id'].apply(lambda x: os.path.join(AUDIO_PATH, f\"{x}.wav\"))\n\nprint(\"Cell 1 Complete. train_df is created.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Environment Setup\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\" # Force-disable wandb from the start\n\n# --- 1. Pin Protobuf ---\nprint(\"Pinning protobuf==3.20.3...\")\n!pip install protobuf==3.20.3 -q\n\n# --- 2. Pin Datasets ---\nprint(\"Pinning datasets==2.16.1...\")\n!pip install datasets==2.16.1 -q\n\n# --- 3. Install all other libraries ---\nprint(\"Installing transformers, librosa, accelerate, and other libs...\")\n!pip install transformers soundfile librosa jiwer 'accelerate>=0.26.0' evaluate -q\n\nprint(\"\\n--- All dependencies installed. Cell 2 Complete. ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: Load, Filter, & Process\nimport pandas as pd\nimport os\nimport re\nimport numpy as np\nimport torch\nimport warnings\nimport librosa\nimport jiwer\nfrom datasets import Dataset, DatasetDict, Audio\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, AutoConfig\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n# --- 1. Set Single GPU ---\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n\n# --- 2. Setup Device ---\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Running on device: {DEVICE}\")\nwarnings.filterwarnings(\"ignore\")\n\n# --- 3. Load Model & Processor (WITH DROPOUT) ---\nprint(\"Loading model and processor (Plan B: collabora/whisper-base-hindi + dropout)...\")\nMODEL_ID = \"collabora/whisper-base-hindi\"\n\n# --- Add dropout ---\nconfig = AutoConfig.from_pretrained(MODEL_ID)\nconfig.dropout = 0.1\n# ---\n\n# These must be global for the next cells\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    MODEL_ID,\n    config=config  # <-- Pass our modified config to the model\n).to(DEVICE)\nprocessor.tokenizer.set_prefix_tokens(language=\"hindi\", task=\"transcribe\")\n\n# --- 4. Filter Missing Files ---\nprint(\"Checking for missing audio files...\")\ndef check_file_exists(filepath):\n    return os.path.exists(filepath)\ntrain_df['file_exists'] = train_df['audio_file'].apply(check_file_exists)\ntrain_df_filtered = train_df[train_df['file_exists'] == True]\nprint(f\"Filtered from {len(train_df)} to {len(train_df_filtered)} samples.\")\n\n# --- 5. Create Dataset ---\ntrain_data, val_data = train_test_split(train_df_filtered, test_size=0.1, random_state=42)\nds = DatasetDict()\nds[\"train\"] = Dataset.from_pandas(train_data.reset_index(drop=True))\nds[\"test\"] = Dataset.from_pandas(val_data.reset_index(drop=True))\n\n# --- 6. Preprocessing Function (with librosa) ---\ndef prepare_dataset(batch):\n    filepath = batch[\"audio_file\"]\n    speech_array, sampling_rate = librosa.load(filepath, sr=16000)\n    \n    batch[\"input_features\"] = processor(speech_array, sampling_rate=16000).input_features[0] \n    batch[\"labels\"] = processor(text=batch[\"clean_transcript\"]).input_ids \n    \n    return batch\n\nprint(\"Applying preprocessing to all samples (this will take a few minutes)...\")\nprocessed_ds = ds.map(prepare_dataset, remove_columns=ds[\"train\"].column_names)\nprint(\"Cell 3 Complete. Data is ready.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Cell 4: Training ---\nimport torch\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nimport evaluate\n\n# 1. Define the Data Collator\n# (This acts like a tetris player, stacking audio of different lengths together)\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # Split inputs and labels\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n        # Pad audio (inputs)\n        batch = processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # Pad text (labels)\n        labels_batch = processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # Replace padding with -100 so the model ignores it when calculating loss\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # If there's a start token at the beginning, remove it\n        if (labels[:, 0] == processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n        return batch\n\n# 2. Define the Metric (WER - Word Error Rate)\nmetric = evaluate.load(\"wer\")\n\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n\n    # Replace -100 with the pad_token_id\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n\n    # Decode predictions and labels to text\n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n\n    # Calculate WER\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n    return {\"wer\": wer}\n\n# 3. Training Arguments (The Rules)\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-finetuned-hindi\",\n    per_device_train_batch_size=8,      # Reduce to 4 if you run out of memory\n    gradient_accumulation_steps=2,      # Helps simulate a larger batch size\n    learning_rate=1e-5,\n    warmup_steps=50,\n    max_steps=500,                      # Train for 500 steps (adjust as needed)\n    gradient_checkpointing=True,        # Saves memory\n    fp16=True,                          # Use half-precision (faster on GPU)\n    evaluation_strategy=\"steps\",\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    generation_max_length=225,\n    save_steps=250,\n    eval_steps=250,\n    logging_steps=25,\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,            # Lower WER is better!\n    push_to_hub=False,\n)\n\n# 4. Initialize Trainer\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=processed_ds[\"train\"],\n    eval_dataset=processed_ds[\"test\"],\n    data_collator=DataCollatorSpeechSeq2SeqWithPadding(processor=processor),\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)\n\n# 5. TRAIN! ðŸš€\nprint(\"Starting training...\")\ntrainer.train()\n\n# 6. Save the Model\nprint(\"Saving model to 'best_model_final'...\")\ntrainer.save_model(\"best_model_final\")\nprocessor.save_pretrained(\"best_model_final\")\nprint(\"Training Complete and Model Saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Submission Generation (Hybrid Strategy)\nimport pandas as pd\nimport os\nimport re\nimport numpy as np\nimport torch\nimport warnings\nimport librosa\nimport jiwer\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\nfrom tqdm.auto import tqdm\n# No Colab imports needed\n\nprint(\"--- Starting Submission Generation (Hybrid Strategy) ---\")\n\n# --- 1. Re-define Global Variables ---\nBASE_PATH = \"/kaggle/input/nppe-2-automatic-disfluency-restoration\" # <-- Kaggle Path\nAUDIO_PATH = os.path.join(BASE_PATH, \"downloaded_audios\") # <-- Kaggle Path\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_PATH = \"best_model_final\" # <-- Loads from /kaggle/working/\n\nprint(f\"Loading trained model from: {MODEL_PATH}\")\nprint(f\"Using device: {DEVICE}\")\n\n# --- 2. Load Saved Model and Processor ---\ntry:\n    processor = AutoProcessor.from_pretrained(MODEL_PATH)\n    model = AutoModelForSpeechSeq2Seq.from_pretrained(MODEL_PATH).to(DEVICE)\nexcept EnvironmentError:\n    print(f\"Error: Model not found at '{MODEL_PATH}'.\")\n    raise\n\n# --- 3. Re-define Collator (Simpler, no labels needed) ---\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n        return batch\n\n# --- 4. Initialize a new Trainer *just for prediction* ---\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./prediction_temp\", # <-- Saves to /kaggle/working/\n    per_device_eval_batch_size=4, \n    fp16=True, \n    predict_with_generate=True,\n    generation_max_length=225,\n    report_to=\"none\",\n)\n\npredictor = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    data_collator=DataCollatorSpeechSeq2SeqWithPadding(processor=processor),\n    tokenizer=processor,\n)\nprint(\"Predictor is loaded with the best model.\")\n\n# --- 5. Load Test Data AND FIND NULLS ---\nTEST_CSV_PATH = os.path.join(BASE_PATH, \"test.csv\")\ntest_df = pd.read_csv(TEST_CSV_PATH)\ntest_df['audio_file'] = test_df['id'].apply(lambda x: os.path.join(AUDIO_PATH, f\"{x}.wav\"))\n\n# Find only the rows where transcript is null\nsamples_to_predict = test_df[test_df['transcript'].isnull()].copy()\n\nprint(f\"Found {len(samples_to_predict)} samples to predict.\")\n\n# --- 6. Process and Predict ONLY the Null Samples ---\nif not samples_to_predict.empty:\n    test_dataset = Dataset.from_pandas(samples_to_predict)\n\n    def prepare_test_dataset(batch):\n        filepath = batch[\"audio_file\"]\n        # Check if file exists, if not, create silent audio as fallback\n        if not os.path.exists(filepath):\n            print(f\"Warning: File not found {filepath}. Using silent audio.\")\n            speech_array = np.zeros(16000) # 1 second of silence\n        else:\n            speech_array, sampling_rate = librosa.load(filepath, sr=16000)\n            \n        batch[\"input_features\"] = processor(speech_array, sampling_rate=16000).input_features[0]\n        return batch\n\n    print(\"Applying preprocessing to test set...\")\n    test_ds_processed = test_dataset.map(prepare_test_dataset, remove_columns=test_dataset.column_names)\n\n    print(f\"Generating predictions for {len(test_ds_processed)} samples...\")\n    predictions = predictor.predict(test_ds_processed)\n    predicted_ids = predictions.predictions\n\n    print(\"Decoding predictions...\")\n    decoded_predictions = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n    \n    # Create a map of {id: predicted_transcript}\n    results_map = {samples_to_predict.iloc[i]['id']: decoded_predictions[i] for i in range(len(decoded_predictions))}\n    \n    # --- 7. Create Submission File (HYBRID) ---\n    print(\"Merging predictions with original test.csv...\")\n    \n    # Create a new column for our predictions\n    test_df['predicted_transcript'] = test_df['id'].map(results_map)\n    \n    # Use the new prediction ONLY if the original was null. Otherwise, keep the original.\n    test_df['transcript'] = test_df['transcript'].fillna(test_df['predicted_transcript'])\n    \n    submission_df = test_df[['id', 'transcript']]\n    \nelse:\n    print(\"No null samples found. Creating submission from original test.csv.\")\n    submission_df = test_df[['id', 'transcript']]\n\n\n# --- 8. Save ---\n# This saves the file to /kaggle/working/submission.csv\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"\\nðŸŽ‰ðŸŽ‰ðŸŽ‰ --- Submission.csv is ready! (Hybrid Strategy) --- ðŸŽ‰ðŸŽ‰ðŸŽ‰\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}